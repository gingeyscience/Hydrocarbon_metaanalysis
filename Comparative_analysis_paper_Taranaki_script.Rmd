---
title: "Kelsey OG reservoir"
output: html_document
date: "2024-01-08"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Loading Packages
```{r}
library(dada2); packageVersion("dada2") # the dada2 pipeline
library(ShortRead); packageVersion("ShortRead") # dada2 depends on this
library(dplyr); packageVersion("dplyr") # for manipulating data
library(tidyr); packageVersion("tidyr") # for creating the final graph at the end of the pipeline
library(Hmisc); packageVersion("Hmisc") # for creating the final graph at the end of the pipeline
library(ggplot2); packageVersion("ggplot2") # for creating the final graph at the end of the pipeline
library(Biostrings)
```
# File Setup
```{r}
# Set up pathway to idemp (demultiplexing tool) and test
idemp <- "/usr/local/bin/idemp" # This is the path to idemp on the UC server
system2(idemp) # Check that idemp is in your path and you can run shell commands from R

# Set up pathway to cutadapt (primer trimming tool) and test
cutadapt <- "/usr/local/bin/cutadapt" # This is the path to cutadapt on the UC server
system2(cutadapt, args = "--version") # Check by running shell command from R

# Set path to shared data folder and contents
data.fp <- "/home/xphiles/data/Kelsey/og_gtdb" 

# List all files in shared folder to check path
list.files(data.fp)
```

# Set up file paths in YOUR directory where you want data; 
```{r}
project.fp <- "/home/xphiles/data/Kelsey/og_gtdb" # CHANGE ME to project directory; don't append with a "/" 

#If your data has already been demultiplexed place all of your files into a directory within the project folder and use the following code:
demultiplex.fp<- "/home/xphiles/data/Kelsey/og_gtdb"

# Set up names of sub directories to stay organized
preprocess.fp <- file.path(project.fp, "01_preprocess")
    filtN.fp <- file.path(preprocess.fp, "filtN")
    trimmed.fp <- file.path(preprocess.fp, "trimmed")
filter.fp <- file.path(project.fp, "02_filter") 
table.fp <- file.path(project.fp, "03_tabletax") 
table.fp.trial <- file.path(project.fp, "04_tabletax")

list.files(demultiplex.fp)

# These directories will not be created until the steps where they are first used. If you have to re-start your pipeline for some reason, or re-run steps, R will just overwrite the files at each step
```

```{r}
# Get full paths for all files and save them for downstream analyses
# Forward and reverse fastq filenames have format: 
fnFs <- sort(list.files(data.fp, pattern="R1.", full.names = TRUE))
fnRs <- sort(list.files(data.fp, pattern="R2.", full.names = TRUE))

#fnFs <- as.data.frame(fnFs)
```

#### Pre-filter to remove sequence reads with Ns

```{r}
# Name the N-filtered files to put them in filtN/ subdirectory
fnFs.filtN <- file.path(preprocess.fp, "filtN", basename(fnFs))
fnRs.filtN <- file.path(preprocess.fp, "filtN", basename(fnRs))

# Filter Ns from reads and put them into the filtN directory
filterAndTrim(fnFs, fnFs.filtN, fnRs, fnRs.filtN, maxN = 0, multithread = TRUE) 
```

```{r}
# Set up the primer sequences to pass along to cutadapt
FWD <- "GTGCCAGCMGCCGCGGTAA"  ## CHANGE ME # this is 515f
REV <- "GGACTACVSGGGTATCTAAT"  ## CHANGE ME # this is 806Br

# Write a function that creates a list of all orientations of the primers
allOrients <- function(primer) {
    # Create all orientations of the input sequence
    require(Biostrings)
    dna <- DNAString(primer)  # The Biostrings works w/ DNAString objects rather than character vectors
    orients <- c(Forward = dna, Complement = Biostrings::complement(dna), Reverse = Biostrings::reverse(dna),
        RevComp = Biostrings::reverseComplement(dna))
    return(sapply(orients, toString))  # Convert back to character vector
}

# Save the primer orientations to pass to cutadapt
FWD.orients <- allOrients(FWD)
REV.orients <- allOrients(REV)
FWD.orients

# Write a function that counts how many time primers appear in a sequence
primerHits <- function(primer, fn) {
    # Counts number of reads in which the primer is found
    nhits <- vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE)
    return(sum(nhits > 0))
}
```

#Before running cutadapt, we will look at primer detection for the first sample, as a check. There may be some primers here, we will remove them below using cutadapt.
```{r}
rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.filtN[[1]]), 
      FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.filtN[[1]]), 
      REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.filtN[[1]]), 
      REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.filtN[[1]]))
```
#### Remove primers with cutadapt and assess the output
```{r}
# Create directory to hold the output from cutadapt
if (!dir.exists(trimmed.fp)) dir.create(trimmed.fp)
fnFs.cut <- file.path(trimmed.fp, basename(fnFs))
fnRs.cut <- file.path(trimmed.fp, basename(fnRs))

# Save the reverse complements of the primers to variables
FWD.RC <- dada2:::rc(FWD)
REV.RC <- dada2:::rc(REV)

##  Create the cutadapt flags ##
# Trim FWD and the reverse-complement of REV off of R1 (forward reads)
R1.flags <- paste("-g", FWD, "-a", REV.RC, "--minimum-length 50") 

# Trim REV and the reverse-complement of FWD off of R2 (reverse reads)
R2.flags <- paste("-G", REV, "-A", FWD.RC, "--minimum-length 50") 

# Run Cutadapt
for (i in seq_along(fnFs)) {
    system2(cutadapt, args = c(R1.flags, R2.flags, "-n", 2, # -n 2 required to remove FWD and REV from reads
                               "-o", fnFs.cut[i], "-p", fnRs.cut[i], # output files
                               fnFs.filtN[i], fnRs.filtN[i])) # input files
}

# As a sanity check, we will check for primers in the first cutadapt-ed sample:
    ## should all be zero!
rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.cut[[1]]), 
      FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.cut[[1]]), 
      REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.cut[[1]]), 
      REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.cut[[1]]))
```
# Now start DADA2 pipeline
```{r}
# Put filtered reads into separate sub-directories for big data workflow
dir.create(filter.fp)
    subF.fp <- file.path(filter.fp, "preprocessed_F") 
    subR.fp <- file.path(filter.fp, "preprocessed_R") 
dir.create(subF.fp)
dir.create(subR.fp)

# Move R1 and R2 from trimmed to separate forward/reverse sub-directories
fnFs.Q <- file.path(subF.fp,  basename(fnFs)) 
fnRs.Q <- file.path(subR.fp,  basename(fnRs))

file.rename(from = fnFs.cut, to = fnFs.Q)
file.rename(from = fnRs.cut, to = fnRs.Q)

# File parsing; create file names and make sure that forward and reverse files match
filtpathF <- file.path(subF.fp, "filtered") # files go into preprocessed_F/filtered/
filtpathR <- file.path(subR.fp, "filtered") # ...
fastqFs <- sort(list.files(subF.fp, pattern=".fq"))
fastqRs <- sort(list.files(subR.fp, pattern=".fq"))
if(length(fastqFs) != length(fastqRs)) stop("Forward and reverse files do not match.")
```
### 1. FILTER AND TRIM FOR QUALITY
```{r}
# If the number of samples is 20 or less, plot them all, otherwise, just plot 20 randomly selected samples
if( length(fastqFs) <= 20) {
  plotQualityProfile(paste0(subF.fp, "/", fastqFs))
  plotQualityProfile(paste0(subR.fp, "/", fastqRs))
} else {
  rand_samples <- sample(size = 20, 1:length(fastqFs)) # grab 20 random samples to plot
  fwd_qual_plots <- plotQualityProfile(paste0(subF.fp, "/", fastqFs[rand_samples]))
  rev_qual_plots <- plotQualityProfile(paste0(subR.fp, "/", fastqRs[rand_samples]))
}

```

```{r}
fwd_qual_plots
```

```{r}
rev_qual_plots
```

```{r}
# write plots to disk
saveRDS(fwd_qual_plots, paste0(filter.fp, "/fwd_qual_plots.rds"))
saveRDS(rev_qual_plots, paste0(filter.fp, "/rev_qual_plots.rds"))

ggsave(plot = fwd_qual_plots, filename = paste0(filter.fp, "/fwd_qual_plots.png"), 
       width = 10, height = 10, dpi = "retina")
ggsave(plot = rev_qual_plots, filename = paste0(filter.fp, "/rev_qual_plots.png"), 
       width = 10, height = 10, dpi = "retina")
```

#### Filter the data
```{r}
filt_out <- filterAndTrim(fwd=file.path(subF.fp, fastqFs), filt=file.path(filtpathF, fastqFs),
              rev=file.path(subR.fp, fastqRs), filt.rev=file.path(filtpathR, fastqRs),
              truncLen=c(200,180), maxEE=c(2,2), truncQ=2, maxN=0, rm.phix=TRUE,
              compress=TRUE, verbose=TRUE, multithread=TRUE)

# look at how many reads were kept
head(filt_out)

# summary of samples in filt_out by percentage
filt_out %>% 
  data.frame() %>% 
  mutate(Samples = rownames(.),
         percent_kept = 100*(reads.out/reads.in)) %>%
  select(Samples, everything()) %>%
  summarise(min_remaining = paste0(round(min(percent_kept), 2), "%"), 
            median_remaining = paste0(round(median(percent_kept), 2), "%"),
            mean_remaining = paste0(round(mean(percent_kept), 2), "%"), 
            max_remaining = paste0(round(max(percent_kept), 2), "%"))
```
# Plot quality
```{r}
# figure out which samples, if any, have been filtered out
remaining_samplesF <-  fastqFs[rand_samples][
  which(fastqFs[rand_samples] %in% list.files(filtpathF))] # keep only samples that haven't been filtered out
remaining_samplesR <-  fastqRs[rand_samples][
  which(fastqRs[rand_samples] %in% list.files(filtpathR))] # keep only samples that haven't been filtered out
fwd_qual_plots_filt <- plotQualityProfile(paste0(filtpathF, "/", remaining_samplesF))
rev_qual_plots_filt <- plotQualityProfile(paste0(filtpathR, "/", remaining_samplesR))

fwd_qual_plots_filt
rev_qual_plots_filt

# write plots to disk
saveRDS(fwd_qual_plots_filt, paste0(filter.fp, "/fwd_qual_plots_filt.rds"))
saveRDS(rev_qual_plots_filt, paste0(filter.fp, "/rev_qual_plots_filt.rds"))

ggsave(plot = fwd_qual_plots_filt, filename = paste0(filter.fp, "/fwd_qual_plots_filt.png"), 
       width = 10, height = 10, dpi = "retina")
ggsave(plot = rev_qual_plots_filt, filename = paste0(filter.fp, "/rev_qual_plots_filt.png"), 
       width = 10, height = 10, dpi = "retina")
```

### 2. INFER sequence variants
```{r}
# File parsing
filtFs <- list.files(filtpathF, pattern=".fq", full.names = TRUE)
#a <- as.data.frame(filtFs)
filtRs <- list.files(filtpathR, pattern=".fq", full.names = TRUE)

# Sample names in order
sample.names <- substring(basename(filtFs), regexpr("", basename(filtFs))) # doesn't drop fastq.gz
sample.names <- gsub("_1.R1.fq", "", sample.names)
#b <- as.data.frame(sample.names)
sample.namesR <- substring(basename(filtRs), regexpr("", basename(filtRs))) # doesn't drop fastq.gz
sample.namesR <- gsub("_1.R2.fq", "", sample.namesR)

# Double check
if(!identical(sample.names, sample.namesR)) stop("Forward and reverse files do not match.")
names(filtFs) <- sample.names
names(filtRs) <- sample.names
```

#### Learn the error rates
```{r}
set.seed(100) # set seed to ensure that randomized steps are replicatable

# Learn forward error rates (Notes: randomize default is FALSE)
errF <- learnErrors(filtFs, nbases = 1e10, multithread = TRUE, randomize = TRUE)

# Learn reverse error rates
errR <- learnErrors(filtRs, nbases = 1e10, multithread = TRUE, randomize = TRUE)

```
#### Plot Error Rates
```{r}
errF_plot <- plotErrors(errF, nominalQ = TRUE)
errR_plot <- plotErrors(errR, nominalQ = TRUE)

errF_plot
# you will get an error saying :Transformation introduced infinite values in continuous y-axis" This is normal, just ignore

errR_plot
# you will get an error saying :Transformation introduced infinite values in continuous y-axis" This is normal, just ignore

# write to disk
saveRDS(errF_plot, paste0(filtpathF, "/errF_plot.rds"))
saveRDS(errR_plot, paste0(filtpathR, "/errR_plot.rds"))

```

#### Dereplication, sequence inference, and merging of paired-end reads
For more information about these steps, see the tutorial on the github. For most of the Stott lab projects, you will want to use Samples Pooled

Nick says to swap pool=T for pool=pseudo. this will preserve rare taxa

#### Default: SAMPLES POOLED 
```{r}
# same steps, not in loop

# Dereplicate forward reads
derepF.p <- derepFastq(filtFs)
names(derepF.p) <- sample.names
# Infer sequences for forward reads
dadaF.p <- dada(derepF.p, err = errF, multithread = TRUE, pool = TRUE)
names(dadaF.p) <- sample.names

# Dereplicate reverse reads
derepR.p <- derepFastq(filtRs)
names(derepR.p) <- sample.names
# Infer sequences for reverse reads
dadaR.p <- dada(derepR.p, err = errR, multithread = TRUE, pool = TRUE)
names(dadaR.p) <- sample.names

# Merge reads together
mergers <- mergePairs(dadaF.p, derepF.p, dadaR.p, derepR.p)
```



#### Construct sequence table
```{r}
seqtab <- makeSequenceTable(mergers)

# Save table as an r data object file
dir.create(table.fp)
saveRDS(seqtab, paste0(table.fp, "/seqtab.rds"))
```

### 3. REMOVE Chimeras and ASSIGN Taxonomy
```{r}
# Read in RDS 
st.all <- readRDS(paste0(table.fp, "/seqtab.rds"))

# Remove chimeras
seqtab.nochim <- removeBimeraDenovo(st.all, method="consensus", multithread=TRUE)

# Print percentage of our seqences that were not chimeric.
100*sum(seqtab.nochim)/sum(seqtab)

getN <- function(x) sum(getUniques(x))
track <- cbind(filt_out, sapply(dadaF.p, getN), sapply(dadaR.p, getN), sapply(mergers, getN), rowSums(seqtab.nochim))
# If processing a single sample, remove the sapply calls: e.g. replace sapply(dadaFs, getN) with getN(dadaFs)
colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")
rownames(track) <- sample.names
track <- track %>% as.data.frame()
track$lost <- track$nonchim/track$input


# Assign taxonomy
tax <- assignTaxonomy(seqtab.nochim, "/home/xphiles/databases/silva_nr99_v138.1_train_set.fa", tryRC = TRUE, multithread=TRUE, minBoot = 80, outputBootstraps = TRUE) 
taxa <- assignTaxonomy(seqtab.nochim, "/home/xphiles/databases/silva_nr99_v138.1_train_set.fa", tryRC = TRUE, multithread=TRUE, minBoot = 80, outputBootstraps = F) 
# This uses the silva database v138. If you need a different database you will have to add it to the "/home/xphiles/databases" folder

taxa <- addSpecies(taxa, "/home/xphiles/databases/silva_species_assignment_v138.1.fa.gz")

# Write results to disk
saveRDS(seqtab.nochim, paste0(table.fp, "/seqtab_final.rds"))
saveRDS(tax, paste0(table.fp, "/tax_final.rds"))
saveRDS(taxa, paste0(table.fp, "/tax_final_species.rds"))
```

### 4. FORMAT OUTPUT to obtain ASV IDs and repset, and input for mctoolsr
```{r}
# Flip table
seqtab.t <- as.data.frame(t(seqtab.nochim))

# Pull out ASV repset
rep_set_ASVs <- as.data.frame(rownames(seqtab.t))
rep_set_ASVs <- mutate(rep_set_ASVs, ASV_ID = 1:n())
rep_set_ASVs$ASV_ID <- sub("^", "ASV_", rep_set_ASVs$ASV_ID)
rep_set_ASVs$ASV <- rep_set_ASVs$`rownames(seqtab.t)` 
rep_set_ASVs$`rownames(seqtab.t)` <- NULL

# Add ASV numbers to table
rownames(seqtab.t) <- rep_set_ASVs$ASV_ID

# Add ASV numbers to taxonomy
taxonomy <- as.data.frame(tax)
taxonomy$ASV <- as.factor(rownames(taxonomy))
taxonomy <- merge(rep_set_ASVs, taxonomy, by = "ASV")
rownames(taxonomy) <- taxonomy$ASV_ID


# Write repset to fasta file
# create a function that writes fasta sequences
writeRepSetFasta<-function(data, filename){
  fastaLines = c()
  for (rowNum in 1:nrow(data)){
    fastaLines = c(fastaLines, as.character(paste(">", data[rowNum,"name"], sep = "")))
    fastaLines = c(fastaLines,as.character(data[rowNum,"seq"]))
  }
  fileConn<-file(filename)
  writeLines(fastaLines, fileConn)
  close(fileConn)
}

# Arrange the taxonomy dataframe for the writeRepSetFasta function
taxonomy_for_fasta <- taxonomy %>%
  unite("TaxString", c("tax.Kingdom", "tax.Phylum", "tax.Class", "tax.Order","tax.Family", "tax.Genus", "boot.Kingdom", "boot.Phylum", "boot.Class", "boot.Order", "boot.Family", "boot.Genus", "ASV_ID"), 
        sep = ";", remove = FALSE) %>%
  unite("name", c("ASV_ID", "TaxString"), 
        sep = " ", remove = TRUE) %>%
  select(ASV, name) %>%
  rename(seq = ASV)

# write fasta file
writeRepSetFasta(taxonomy_for_fasta, paste0(table.fp, "/repset_bootstrap.fasta"))

taxonomy_species <- as.data.frame(taxa)
taxonomy_species$ASV <- as.factor(rownames(taxonomy_species))
taxonomy_species <- merge(rep_set_ASVs, taxonomy_species, by = "ASV")
rownames(taxonomy_species) <- taxonomy_species$ASV_ID

taxonomy_species_for_fasta <- taxonomy_species %>%
  unite("TaxString", c("Kingdom", "Phylum", "Class", "Order","Family", "Genus", "Species", "ASV_ID"), 
        sep = ";", remove = FALSE) %>%
  unite("name", c("ASV_ID", "TaxString"), 
        sep = " ", remove = TRUE) %>%
  select(ASV, name) %>%
  rename(seq = ASV)

# write fasta file
writeRepSetFasta(taxonomy_species_for_fasta, paste0(table.fp, "/repset_species.fasta"))



# Also export files as .txt
write.table(seqtab.t, file = paste0(table.fp, "/seqtab_final.txt"),
            sep = "\t", row.names = TRUE, col.names = NA)
```

5. Make a tree for unifrac distance 

Remove NA at phylum level as you will do this anyway

```{r}
taxonomy_species_tree <- taxonomy_species %>% drop_na("Phylum")
taxonomy_species__tree_for_fasta <- taxonomy_species_tree %>%
  unite("TaxString", c("Kingdom", "Phylum", "Class", "Order","Family", "Genus", "Species", "ASV_ID"), 
        sep = ";", remove = FALSE) %>%
  unite("name", c("ASV_ID", "TaxString"), 
        sep = " ", remove = TRUE) %>%
  select(ASV, name) %>%
  rename(seq = ASV)

writeRepSetFasta(taxonomy_species__tree_for_fasta, paste0(table.fp, "/repset_species_no_phylum.fasta"))

library(baseq)
library(DECIPHER); packageVersion("DECIPHER")

seqs <- readDNAStringSet( "/home/xphiles/data/Kelsey/og_gtdb/03_tabletax/repset_species_no_phylum.fasta")

seqs <- OrientNucleotides(seqs)
aligned <- AlignSeqs(seqs)
head(aligned)

writeXStringSet(aligned, paste0(table.fp, "/aligned_species.fasta"))

library(phangorn); packageVersion("phangorn")

alignment <- read.phyDat("/home/xphiles/data/Kelsey/og_gtdb/03_tabletax/aligned_species.fasta", format = "fasta", type = "DNA")

##Alignment is an object of class phyDat

dm <- dist.ml(alignment)
tree <- NJ(dm)
write.tree(tree, paste0(table.fp, "/tree_species.fasta"))
write.tree(tree, paste0(table.fp, "/tree_species.tre"))

```


